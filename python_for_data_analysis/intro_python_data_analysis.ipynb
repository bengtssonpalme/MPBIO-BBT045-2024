{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47ba5c3-19e2-452d-ae67-3c10a7e1d879",
   "metadata": {},
   "source": [
    "# BBT045: Intro to Python programming for data analysis\n",
    "\n",
    "Author: Vi Varga (Based on work of: Filip Buric)\n",
    "\n",
    "Last Updated: 24.01.2025\n",
    "\n",
    "\n",
    "## Intro\n",
    "\n",
    "This module will be relying on the following material, which you are encouraged to read more extensively beyond the requirement for this course.\n",
    "\n",
    "Becoming comfortable (not only familiar) with a programming language is like a superpower and you'll find it enables you to do many more things beyond the initial context (and enhances your employment outlook).\n",
    "\n",
    "- [*An Introduction to Programming for Bioscientists: A Python-Based Primer*](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004867) by Ekmekci et al.\n",
    "    - This PLoS article is a good overview/introduction to the use of Python for data science. \n",
    "    - Some aspects may seem confusing in the beginning, but Python (like all languages, programming or otherwise) is best learned through _use_. As you work your way through this course, this article can be used a reference. \n",
    "\n",
    "- [*Python for Data Analysis, 3E*](https://wesmckinney.com/book/) by Wes McKinney\n",
    "    - An alternative option for exploring the use of Python in the context of data analysis & scientific computing\n",
    "    - Much more detailed than the PLoS article above, but less relevant for bioinformatics - a potentially useful reference\n",
    "\n",
    "- [*Intro to Advanced Python*](https://python-course.eu/advanced-python/) by Bernd Klein\n",
    "    - This website provides resources for some of the more advanced utility of Python, which can improve your overall coding ability. \n",
    "    - The [*Intro to Python Tutorial*](https://python-course.eu/python-tutorial/) portion of the same website has more basic introductory materials, if you like all of your references to be in the same place. \n",
    "\n",
    "- Official documentation pages: You do not need to read these in their entirety, but they can provide helpful information for a variety of the modules we will be using. Here, I will include a few of the basics: \n",
    "    - Python programming language official website: https://www.python.org/\n",
    "    - Pandas documentation: https://pandas.pydata.org/\n",
    "    - Numpy documentation: https://numpy.org/\n",
    "    - Scipy (Python for scientific computing): https://scipy.org/\n",
    "    - Re (regex/regular expressions in Python): https://docs.python.org/3/library/re.html\n",
    "    - Biopython: https://biopython.org/\n",
    "\n",
    "- Miscellaneous :) There are many good resources on the web. We'll point out some when relevant.\n",
    "\n",
    "For this module, please read the sections we point out. You can download a copy of this Jupyter Notebook from [here](intro_python_data_analysis.ipynb) for your own computer. If you wish to download this file directly to Vera, simply run `wget https://bengtssonpalme.github.io/MPBIO-BBT045-2025/python_for_data_analysis/intro_python_data_analysis.ipynb`.\n",
    "\n",
    "\n",
    "## Getting help\n",
    "\n",
    "Most Python modules have good documentation.\n",
    "\n",
    "If you are using the Spyder IDE, which is the Python IDE that comes with an Anaconda installation, you can access help pages for modules, functions and more by clicking on the relevant string with your cursor and entering `CTRL+I`. Find more information on the Spyder documentation pages, here: https://docs.spyder-ide.org/5/panes/help.html\n",
    "\n",
    "In Jupyter Notebook or Jupyter Lab (where this notebook is intended to be used), you can access help pages for functions using `help()` or `?`, as shown in the cell below. \n",
    "- Note that the cell will not execute by default; this is to save on page space. You can test the results that would be output yourself, by removing the `%%script false --no-raise-error` command at the top of the cell ([Reference](https://stackoverflow.com/questions/19309287/how-to-intermittently-skip-certain-cells-when-running-ipython-notebook))\n",
    "- Find more information here: https://problemsolvingwithpython.com/02-Jupyter-Notebooks/02.07-Getting-Help-in-a-Jupyter-Notebook/\n",
    "\n",
    "Googling is of course an alternative - StackOverflow has plenty of very helpful Python users!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e4cba-6eaf-4b51-b0a4-450aff2f24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# using help()\n",
    "import pandas as pd # this is how Pandas is commonly imported\n",
    "help(pd.DataFrame) # DataFrame is a specific function of the Pandas module\n",
    "\n",
    "# using ?\n",
    "pd.Series?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e823a894-ef93-479e-83ef-4944b6cdf2fe",
   "metadata": {},
   "source": [
    "## Prep\n",
    "\n",
    "Begin by creating a new directory for the Python Tutorial and Homework. I used `Python_Intro/`, but the name is up to you - just make sure you name the directory in such a way, that you'll know later what's in it!\n",
    "\n",
    "If you haven't already done so, make sure to download the notebook (instructions above)!\n",
    "\n",
    "In order to complete this tutorial, you will need to copy the following files from `/cephyr/NOBACKUP/groups/bbt045_2025/Resources/Python/`: \n",
    " - `iris.csv`\n",
    " - `Python-Intro-HW.sh`\n",
    "\n",
    "You will also need to set up the custom Jupyter OnDemand we will be using, within your own `$HOME` directory. Run the following command in your terminal: `mkdir -p ~/portal/jupyter/`\n",
    "\n",
    "Then copy the `Python-Intro-HW.sh` file to the `~/portal/jupyter/` directory. You will now be able to select this file as a Runtime option on the Vera OnDemand Portal, located here: https://vera.c3se.chalmers.se/public/root/. Your interactive session setup on the Vera OnDemand portal should look something like this:\n",
    "\n",
    "![Custom JupyterLab setup for Vera OnDemand](./img/vera_ondemand_custom_jupyter.png)\n",
    "\n",
    "Click \"Launch\", and then wait for the resource allocation process to complete. When resources have been allocated, you'll be able to open JupyterLab by clicking \"Connect to Jupyter\":\n",
    "\n",
    "![Vera OnDemand Jupyter ready to launch](./img/vera_ondemand_jupyter_readytolaunch.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df9b51b-db6a-445d-b46d-1296b1d7e6e1",
   "metadata": {},
   "source": [
    "Once all of hte above preparations are done, you can finally get started on the tutorial proper: \n",
    "\n",
    "We will load the necessary modules here, prior to running any commands. As a general rule, it is a good idea to load all required modules at the beginning of a script, and to group them together. Especially when working in a Notebook format (as you are here), including the module loading commands later in the Notebook runs the risk, when you re-run sections, of forgetting to re-run the specific code cell in which the module loading command is found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d97f6f-c649-4cd1-a5c8-2f61fffca14d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd # enables manipulation of dataframes in Python\n",
    "import numpy as np # enables manipulation of arrays in Python\n",
    "import random # fetches random values\n",
    "from urllib.request import urlretrieve # enables the downloading of files from the internet via URL\n",
    "import gzip # module to enable I/O of gzipped files\n",
    "import shutil # module to perform operations on files\n",
    "from Bio import SeqIO # import the SeqIO Biopython module for working with sequence data\n",
    "import matplotlib.pyplot as plt # import the matplotlib graphics module\n",
    "import seaborn as sns # import the seaborn graphics module\n",
    "import warnings # module to manage warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # prevent Python from warning of future feature deprecation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d2714-968d-4368-808a-e704f355c792",
   "metadata": {},
   "source": [
    "## Attributes & Data Types\n",
    "\n",
    "Much like in R, different data types have different attributes in Python. However, these attributes are not nearly as important in Python as they are in R (at least, not for the average user). You can get the attributes of a specific object using the command `dir()`, but the information returned by this command can be quite complicated. \n",
    "\n",
    "Data _types_, on the other hand, matter a lot in Python. Python will usually correctly assess the data type of a specific object, but it's worth checking, especially with data frames, as Python can sometimes make mistakes. The most commonly used data types in python are \"str\" (string), \"int\" (integer) and \"float\". Strings are text (i.e., words, sentences, etc.), integers are numbers without decimals and floats are numbers with decimals. Note that strings can be identified by the fact that they are _always_ printed by Python in quotation marks. \n",
    "\n",
    "In the following cells, you can see a demonstration of how this works. **Note that in Python, variable assignment is done using the format: `VARABLE_NAME = VARIABLE_CONTENT`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d9da7d-f37f-4f20-ae4e-4b3151b18f56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# string\n",
    "test_string = \"test string\"\n",
    "print('The content of variable test_string is the string \"' + test_string + '\".')\n",
    "print(type(test_string))\n",
    "\n",
    "# integer\n",
    "test_int = 3\n",
    "print('The content of variable test_int is the string ' + str(test_int))\n",
    "# note that when numbers are concatenated with strings in a print statement, the number has to be converted to a string with str()\n",
    "print(test_int)\n",
    "print(type(test_int))\n",
    "\n",
    "# float\n",
    "test_float = 3.14159\n",
    "print(\"The content of variable test_float is the string \" + str(test_float))\n",
    "# Note that while Python will return strings in double quotes, you can provide strings to Python in single quotes\n",
    "type(test_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbb1eb-2482-4d66-80e9-a1486fe32480",
   "metadata": {},
   "source": [
    "### Working with Python data structures\n",
    "\n",
    "Base Python uses 4 primary data structures (list, tuple, set, dictionary), and a variety of modules enable users to create others. \n",
    "\n",
    "For our purposes in this course, the most important of these modules are Pandas, which allows Python users to create and manipulate dataframes; and Numpy, which enables the creation and manipulation of arrays. These two data structures (dataframes and arrays) are slightly different, but a number of Numpy array commands can be used to manipulate Pandas dataframes, as well. If you are interested in learning more, you can can learn more about the differences here: https://www.askpython.com/python/pandas-dataframe-vs-numpy-arrays\n",
    "\n",
    "Below, you'll be introduced to how these data structures function in Python. For a quick reference sheet on Python data types and data structures, see here: https://www.w3schools.com/python/python_datatypes.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab23c1f-db49-4fd4-b8ab-b698c09e3b3f",
   "metadata": {},
   "source": [
    "### Lists\n",
    "\n",
    "Lists in Python are exactly what they sound like: a comma-separated _list_ of objects, which you can create by surrounding the the elements of the list with [brackets]. \n",
    "\n",
    "The elements of lists can be changed; and they can contain a variety of different data types. See more information here: https://www.w3schools.com/python/python_lists.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6479840-2b25-45e5-80bd-4fd900d8ad7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list\n",
    "test_list = [\"test\", \"list\", \"example\"]\n",
    "print(test_list)\n",
    "print(type(test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a2cc5-d154-433b-8c4f-c76b605434be",
   "metadata": {},
   "source": [
    "### Tuples\n",
    "\n",
    "Tuples are similar to lists, except that their contents are static (i.e., the contents cannot be changed). You can create a tuple by including a comma-separated list of objects in (parentheses). See more information here: https://www.w3schools.com/python/python_tuples.asp\n",
    "\n",
    "The elements of a tuple are indexed. **Note that indexing in Python starts with 0, and index values should be contained in [brackets].**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced9006-fa7e-46d9-93f1-45402732793e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuple\n",
    "test_tuple = (\"test\", \"tuple\", \"example\")\n",
    "print(test_tuple)\n",
    "print(type(test_tuple))\n",
    "# indexing example\n",
    "print('The second element of the tuple is the word \"' + test_tuple[1] + '\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992181c2-d13c-4587-9a24-5d4d391b73f2",
   "metadata": {},
   "source": [
    "### Set\n",
    "\n",
    "Sets are also similar to lists, in that they are 1-dimensional and include multiple values. Like tuples, their contents cannot be changed; though unlike tuples, objects can be added to or removed from a set. Also unlike tuples, sets are unordered, so the elements cannot be indexed. See more information here: https://www.w3schools.com/python/python_sets.asp\n",
    "\n",
    "Sets are created by including a comma-separated list of objects in {braces}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d8343-58ae-4c54-bb29-3c40e7b1615d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set\n",
    "test_set = {\"test\", \"set\", \"example\"}\n",
    "print(test_set)\n",
    "# note how this was likely printed to your screen in a different order from how the object were originally given to the set\n",
    "print(type(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eec380-a078-4b32-9f62-3f056095bf0c",
   "metadata": {},
   "source": [
    "### Dictionary\n",
    "\n",
    "Dictionaries are perhaps the most useful inbuilt data structure in Python. They consist of a series of key:value pairs, that allow you to store information with references. See more information here: https://www.w3schools.com/python/python_dictionaries.asp\n",
    "\n",
    "Below, the basics of dictionary creation and use are demonstrated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6a1b94-c64d-4789-9d89-b1d56db49b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dictionary\n",
    "test_dict = {\"key1\": \"value1\",\n",
    "             \"key2\": \"value2\",\n",
    "             \"key3\": \"value3\"}\n",
    "print(test_dict)\n",
    "print(type(test_dict))\n",
    "\n",
    "# reference a specific value in a dictionary with DICTIONARY_NAME[KEY]\n",
    "print('The value associated with key2 is \"' + test_dict[\"key2\"] + '\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf93766c-7eaf-4f71-9df8-379dbf81809e",
   "metadata": {},
   "source": [
    "## Iterating over data structures\n",
    "\n",
    "Data structures are important for _storing_ data, but will not be of much use if you do not know how to actually extract data from them. Below are a few examples of `if`/`else` statements, `for` loops and `while` loops in Python, using some of the data structures introduced above. \n",
    "\n",
    "One thing to note before you begin coding in Python, is that **indentation matters**. Unlike in R, the end of a particular code block is defined not by the closing of parentheses, but by the end of a specific level of indentation. If you have problems getting code that seems right to you to work, one of the first things to check should be whether your indentation is correct. For example, if one line is indented using a tab (\"\\t\"), and the next line is indented using 4 spaces (\"   \"), Python will return an error that may be less than helpful, depending on your IDE. \n",
    "\n",
    "### `if` statements\n",
    "\n",
    "If statements in Python test the truth of a condition. If the condition evaluetes to `True`, then the command(s) within the `if` statement are executed. Note that successive if statements are evaluated independently of one another, so be careful about putting them in a row. If you wish to test a secondary condition (i.e., one or multiple alternatives to the first `if` statement), use `elif` for the alternative condition tests. An `else` statement cannot be combined with an alternative condition, so using `else` will finish the section of code started by the previous `if` statement made at that indentation level.\n",
    "\n",
    "In the code block below, an `if` statement is used to evaluate whether a randomly generated integer is greater than (>), less than (<) or equal to (==) 0. See more about conditionals in Python here: https://www.w3schools.com/python/python_conditions.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943169ad-fdcf-45ad-bf5d-3df71558742b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = random.randint(-10, 10) # return a random integer between -10 and 10\n",
    "print(\"The value of k is \" + str(k) + \". Now let's see if the conditional works correctly:\")\n",
    "\n",
    "if k > 0:\n",
    "    # see if the value of k is >0\n",
    "    print(\"The value of k is positive.\")\n",
    "elif k < 0:\n",
    "    # see if the value of k is <0\n",
    "    print(\"The value of k is negative.\")\n",
    "else: \n",
    "    # if neither the primary nor secondary conditions are fulfilled\n",
    "    # i.e., if k==0\n",
    "    print(\"The value of k is 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4814d1-39f6-4dda-a604-40ed3ec27a58",
   "metadata": {},
   "source": [
    "### `for` loops and/vs. `while` loops\n",
    "\n",
    "A `for` loop will iterate over a range of items within an iterable object (ex., the elements of list, the keys in a dictionary, etc.), while a `while` loop will evaluate/continue to loop until the given condition returns `False`.\n",
    "\n",
    "Programmers often learn to use one or the other, and then build their code around the way that their preferred loop type works. Personally, I prefer `for` loops, because in my work as a bioinformatician, I am generally looping through each of the elements in an iterable object (most often a list or dictionary). I prefer to place `if` statements within my `for` loops in order to filter my data, rather than use `while` loops. As a result, most (if not all) of the examples/solutions I provide you will use `for` loops. However, you are free to use `while` loops in your own code, if you wish! It really just comes down to personal preference.\n",
    "\n",
    "- See more about `for` loops here: https://www.w3schools.com/python/python_for_loops.asp\n",
    "- See more about `while` loops here: https://www.w3schools.com/python/python_while_loops.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f1bb9f-b6a3-405d-8c95-4077a756cc3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# while loop\n",
    "\n",
    "# set the iterator j to 0\n",
    "j = 0\n",
    "\n",
    "while j <= 7:\n",
    "    print(j)\n",
    "    j += 1 \n",
    "    # add +1 to the value of j with each iteration of the loop\n",
    "    # the looping will stop once j == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6fdfb9-2bdd-4d41-b053-b4f59ba3c1b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for loop\n",
    "\n",
    "# looping over a list\n",
    "\n",
    "# list of accession numbers for a few RAS proteins\n",
    "# source: https://www.ebi.ac.uk/interpro/entry/pfam/PF00071/protein/UniProt/#table\n",
    "ras_list = [\"A0A010Q0G0\", \"A0A010Q6W4\", \"A0A010Q7K6\", \"A0A010QKF9\", \"A0A010QSB8\"]\n",
    "\n",
    "for ras_prot in ras_list: \n",
    "    # loop over the elements of ras_list\n",
    "    if ras_prot.endswith(\"8\"): \n",
    "        # find a RAS protein accession ending with the number 8\n",
    "        # and print the results\n",
    "        print(\"The RAS protein accession number \" + ras_prot + \" ends with the number 8.\")\n",
    "\n",
    "\n",
    "# looping over a dictionary\n",
    "\n",
    "# dictionary of bacterial species and associated phyla\n",
    "bacteria_dict = {\"Gardnerella vaginalis\": \"Actinobacteria\",\n",
    "                \"Lactobacillus crispatus\": \"Firmicutes\",\n",
    "                \"Lactobacillus iners\": \"Firmicutes\"}\n",
    "\n",
    "# loop over the dictionary to find which species are Firmicutes\n",
    "for key, value in bacteria_dict.items():\n",
    "    # iterate over the items in a dictionary\n",
    "    # using this method may be unnecessary here, but it allows the flexibility of accessing both keys & values\n",
    "    if bacteria_dict[key] == \"Firmicutes\":\n",
    "        # identify dictionary elements where the value is Firmicutes\n",
    "        print(key + \" is a Firmicute (Bacillota).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904deb03-baaf-428d-9b47-8ab495a18f20",
   "metadata": {},
   "source": [
    "## Working with tabular data: Pandas\n",
    "\n",
    "Many datasets can naturally be organized in tables. Unfortunately, Python does not have a built-in method for working with dataframes; this is where the Pandas and Numpy modules come into the picture. \n",
    "\n",
    "We've already imported these modules, so now we will begin by importing a data table into a Pandas dataframe. (Note that `pandas` is generally imported as the abbreviation `pd`, while `numpy` is generally imported as the abbreviation `np`.) In order to run the code below, make sure that the `iris.csv` file (modified from [here](https://ocw.mit.edu/courses/15-097-prediction-machine-learning-and-statistics-spring-2012/resources/iris/) to include column names) is located in the same directory as this .ipynb Jupyter Notebook file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4be87-8002-4371-a0d1-38f7e8bc970b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the file name to a variable\n",
    "iris_db = \"iris.csv\"\n",
    "\n",
    "# import the iris dataset into a Pandas dataframe\n",
    "iris_df = pd.read_csv(iris_db, sep=',', header=0)\n",
    "\n",
    "print(iris_df.columns)\n",
    "\n",
    "# print the dataframe\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b8654-708c-4a31-8246-d48500fd69a8",
   "metadata": {},
   "source": [
    "### Filtering rows based on values\n",
    "\n",
    "Pandas has inbuilt utility that allows you to filter the dataframe. \n",
    "\n",
    "For example, to filter the `iris_df` dataframe to only the rows which have sepal lengths greater than or equal to 5, do the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0127bab-c340-4110-8ec6-769d56276cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_df.loc[iris_df['Sepal_Length'] >= 5]\n",
    "# note that this doesn't save the filtered dataframe, only displays it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910066ea-dd74-4cb4-8592-8929ebde3a88",
   "metadata": {},
   "source": [
    "### Selecting & working on columns\n",
    "\n",
    "To select a column of a Pandas dataframe, use the syntax `DATAFRAME[COLUMN_NAME]`\n",
    "\n",
    "To select multiple columns of a Pandas dataframe, use the syntax `DATAFRAME[[COLUMN_NAME_1, COLUMN_NAME_2]]`\n",
    "\n",
    "A variety of manipulations/transformations can be done on the contents of Pandas columns, including (but very much not limited to):\n",
    "- Mathematical transformations (ex., `dataframe.column.mul(2)` multiplies all values in the selected dataframe column by 2)\n",
    "- Text manipulation\n",
    "- Creating new columns based on existing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de16d3f2-2448-427e-8e97-0009f8ef608f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the log10 values of sepal length\n",
    "print(np.log10(iris_df['Sepal_Length']))\n",
    "# Pandas has inbuilt methods for multiplication mul(), division .div(), addition .add() & substraction .sub()\n",
    "# but for more complicated math, numpy can help\n",
    "print(iris_df['Sepal_Length'].add(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c63b74-ff98-42c0-ac16-06715d4bbcb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# manipulate text in a column\n",
    "iris_df['Class'] = iris_df['Class'].str.replace('Iris-', '')\n",
    "# above, removing the Iris- substring from the Class column and saving the edit to the dataframe\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b76c3-1681-4958-8472-f01d3db33b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# divide two columns by each other\n",
    "# once again, we use Numpy to accomplish this\n",
    "iris_df['Petal_WdivD'] = iris_df['Petal_Length']/iris_df['Petal_Width']\n",
    "# round the new column to 3 decimal places\n",
    "iris_df['Petal_WdivD'] = iris_df['Petal_WdivD'].round(3)\n",
    "\n",
    "# now see the new dataframe\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebe5a5-fefa-4bea-9171-7f50a6ebd040",
   "metadata": {},
   "source": [
    "### Working on groups of values\n",
    "\n",
    "Sometimes data has natural categorizations, that you would like to group the data by, in order to get a feel for the characteristics of a particular group. Pandas allows you to quickly organize your data in such a way using `groupby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247bd5f2-1fed-46d2-b04d-6be1df94c366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using groupby\n",
    "\n",
    "# group the data according to the type of iris and get the means of each type of data for that iris type\n",
    "iris_df.groupby(['Class']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be2af6d-1805-4cd3-bdac-cd3b41554f23",
   "metadata": {},
   "source": [
    "### Lambda functions, etc.\n",
    "\n",
    "So far in this Jupyter Notebook, you have been introduced to the basic functionality and grammar of the Python programming language; as well as to the use of the Pandas module, in particular, as it will be important for this course. However, there of course remain a wide variety of functionalities available in base Python (not to mention, available through all of the many Python modules that exist), to which you have yet to be introduced. It is not within the scope of this course to dive into all of those possible applications. However, I will here briefly introduce you to one more thing.\n",
    "\n",
    "_Lambda functions_ are small anonymous functions that can be used to manipulate data when Python's (or a Python module's) inbuilt functionality doesn't enable you to do what you are trying to. (Anonymous functions are unnamed functions: Essentially, they are functions that exist only within a single line of code, rather than within the broader set of local or global variables.) Writing anonymous functions can be tricky, but it's worth briefly taking a look at the structure of them here, so that you can recognize what they are, if they happen to pop up in a StackOverflow solution to a problem you're trying to solve.\n",
    "\n",
    "Lambda functions take the form: `lambda ARGUMENT(S): EXPRESSION` where the argument(s) are the placeholder variable names for the arguments you are supplying to the function, and the expression is the actual function, itself. An illustrative example is provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdda9c1-981d-4166-ba7e-0d3979c21c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda function\n",
    "\n",
    "# create a Pandas dataframe with lists in one of the columns\n",
    "d = {'sequence_ids': [\"seq_1\", \"seq_2\"], 'sequence_start': [\"ACTGTGTG\", \"ATGTGTG\"], 'functional_profile': [[\"protein folding\", \"chaperone\"], [\"immune\", \"autoimmune\", \"white blood cells\"]]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df)\n",
    "\n",
    "# expand the lists into comma-separated strings using a lambda function\n",
    "df['functional_profile'] = df['functional_profile'].apply(lambda x: ', '.join(map(str, x)))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c932b56-c80b-46d4-9615-a8c529b42eb7",
   "metadata": {},
   "source": [
    "## Application: Reading FASTA files\n",
    "\n",
    "[Biopython](https://biopython.org/) is a collection Python modules built for bioinformaticians and biologists: it contains a variety of modules with functions that enable you to manipulate biological data, as well as open file types unique to biology (ex., FASTA files). While it isn't necessary to use Biopython to do bioinformatics (personally, I only use it on rare, specific occasions), it can be a very useful tool. Modules in the Biopython package can be imported using `import Bio.MODULE` (note the capital \"B\") (see the module import section earlier in this document). For more information, see also the [Biopython Tutorial & Cookbook](https://biopython.org/DIST/docs/tutorial/Tutorial.pdf).\n",
    "\n",
    "Let's load ORFs and protein sequences from a reference yeast genome using the `Bio.SeqIO` module. For more information on this module, check out the [Biopython SeqIO Wiki page](https://biopython.org/wiki/SeqIO) or [ReadTheDocs page](https://biopython.org/docs/1.75/api/Bio.SeqIO.html).\n",
    "\n",
    "Note that this is not the only way to load a FASTA file into Python. You can simply parse/iterate over the file to extract necessary information, transform it into a Pandas dataframe, or (my personal favorite) turn it into a dictionary. An example for loading a FASTA file into a dictionary is given below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5cda1-5849-4cf6-9000-afefda9c0153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for more detailed information on downloading files from the internet with Python, see: \n",
    "# https://realpython.com/python-download-file-from-url/\n",
    "\n",
    "# set necessary variables\n",
    "url = (\"http://sgd-archive.yeastgenome.org/sequence/S288C_reference/orf_dna/orf_genomic_all.fasta.gz\")\n",
    "filename = \"orf_trans.fasta.gz\"\n",
    "\n",
    "# download file\n",
    "urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220047f6-7140-4f98-89af-acb2be226fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the FASTA file & extract the data\n",
    "\n",
    "# we need to start by decompressing the gzipped file because Biopython does not play well with gzipped files\n",
    "# ref: https://github.com/biopython/biopython/issues/1686\n",
    "# we can do this with the gzip module\n",
    "# ref: https://stackoverflow.com/questions/31028815/how-to-unzip-gz-file-using-python\n",
    "with gzip.open(filename, 'rt') as f_in:\n",
    "    # open the gzipped file for reading\n",
    "    # create the output filename based on the input filename\n",
    "    new_file = filename.removesuffix('.gz')\n",
    "    with open(new_file, 'wt') as f_out:\n",
    "        # now open the new file for writing\n",
    "        # and write out the contents\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "# now create a dictionary \n",
    "# ref: https://stackoverflow.com/questions/29333077/reading-a-fasta-file-format-into-python-dictionary\n",
    "yeast_seq_dict = {rec.id : rec.seq for rec in SeqIO.parse(new_file, \"fasta\")}\n",
    "\n",
    "# print the first key and value from the dictionary: a header & sequence\n",
    "# get the first key\n",
    "yeast_key_1 = list(yeast_seq_dict.keys())[0]\n",
    "print(yeast_key_1)\n",
    "print(yeast_seq_dict[yeast_key_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d383e85-5b79-415a-ad7b-5700855fd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false --no-raise-error\n",
    "\n",
    "'''\n",
    "For the sake of introducing Biopython, we created a sequence dictionary using the Bio.SeqIO module in the code above. \n",
    "\n",
    "However, it's worth noting that you can create this type of dictionary on your own, as well. \n",
    "In this code block, which will not execute by default, I demonstrate how that can be done. \n",
    "'''\n",
    "\n",
    "# create an empty dictionary to fill with sequence information\n",
    "seq_dict = {}\n",
    "\n",
    "# iterate over the gzipped file's contents to extract information\n",
    "with gzip.open(filename, \"rt\") as handle:\n",
    "    # open the .gzip file for reading\n",
    "    # create an empty list for the sequence information\n",
    "    seq_list = []\n",
    "    for line in handle: \n",
    "        # iterate through the file line by line\n",
    "        if line.startswith(\">\"): \n",
    "            # identify the sequence header lines\n",
    "            # first add the previous header & sequence to the dictionary\n",
    "            if seq_list:\n",
    "                # if list block contains sequence portions\n",
    "                # concatenate the list into a single sequence\n",
    "                seq_dict[header] = ''.join(seq_list)\n",
    "                # empty the list\n",
    "                seq_list = []\n",
    "            header_full = line.strip()[1:]\n",
    "            # remove the \">\" character with [1:] and the end-line character with .strip()\n",
    "            header = header_full.split()[0]\n",
    "            # save only the sequence identifier to be the new header\n",
    "        else: \n",
    "            # for the sequence lines\n",
    "            seq_list.append(line.strip())\n",
    "    if seq_list:\n",
    "        # for the last sequence line\n",
    "        # need to add the dictionary contents outside the main loop\n",
    "        seq_dict[header] = ''.join(seq_list)\n",
    "\n",
    "# reference for working with multi-line fastas\n",
    "# https://stackoverflow.com/questions/50856538/how-to-convert-multiline-fasta-files-to-singleline-fasta-files-without-biopython\n",
    "\n",
    "# now print the first key:value pair\n",
    "key_1 = list(seq_dict.keys())[0]\n",
    "print(key_1)\n",
    "print(seq_dict[key_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88c673-be5c-43e5-b7ac-393cd077d840",
   "metadata": {},
   "source": [
    "Biopython's greatest advantage over base Python is the ability to perform operations that would be programmatically more complex (and would likely take longer both in terms of the amount of time required to generate the code, and in terms of the number of lines of code) in a quick and straightforward manner.\n",
    "\n",
    "For example, below you can see how to use Biopython to generate the reverse complements of the yeast ORFs; and to translate the original ORF sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48247db8-c99c-4e9a-b765-3f408c586763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reverse complements\n",
    "\n",
    "with open(new_file, \"r\") as infile: \n",
    "    # open the fasta file for reading\n",
    "    counter = 0 \n",
    "    # here I will use a counter to prevent Python from printing every single result\n",
    "    for record in SeqIO.parse(infile, \"fasta\"):\n",
    "        # for each of the records in the fasta file\n",
    "        counter += 1\n",
    "        # add 1 to the counter for each record\n",
    "        if counter <= 10:\n",
    "        # only print the first 10 records\n",
    "            # use SeqIO to find the complement sequences\n",
    "            print( \"For \" + record.id + \" the reverse complement sequence is '\" + str(record.seq.complement()) + \"'.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeae8809-210c-40bf-a484-8eafb7863bb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# translate the ORF sequences\n",
    "\n",
    "with open(new_file, \"r\") as infile: \n",
    "    # open the fasta file for reading\n",
    "    counter = 0 \n",
    "    # here I will use a counter to prevent Python from printing every single result\n",
    "    for record in SeqIO.parse(infile, \"fasta\"):\n",
    "        # for each of the records in the fasta file\n",
    "        counter += 1\n",
    "        # add 1 to the counter for each record\n",
    "        if counter <= 10:\n",
    "        # only print the first 10 records\n",
    "            # use SeqIO to find the complement sequences\n",
    "            print( \"For \" + record.id + \" the reverse complement sequence is '\" + str(record.seq.translate()) + \"'.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff7d9ab-6b74-4c96-8f90-60c1ab77861b",
   "metadata": {},
   "source": [
    "## Application: Working with GFF files\n",
    "\n",
    "The [General feature format](https://en.wikipedia.org/wiki/General_feature_format) contains information about genes and DNA, RNA, and protein sequences. It's structured as a tab-delimited table, meaning we can read it directly into a Pandas dataframe.\n",
    "\n",
    "Here we read in the genome info as a GFF from the source website (the file is not copied, just read directly into the dataframe). We could parse the file to extract the column names, but it's just simpler if we write them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2778986-0af9-4486-a455-d9530b9d76ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reading in a GFF file\n",
    "\n",
    "# set necessary variables\n",
    "gff_url = (\"ftp://ftp.ncbi.nlm.nih.gov/genomes/archive/old_genbank/Bacteria/Halobacterium_sp_uid217/AE004437.gff\")\n",
    "gff_filename = \"AE004437.gff\"\n",
    "\n",
    "# download file\n",
    "urlretrieve(gff_url, gff_filename)\n",
    "\n",
    "\n",
    "# gff files don't contain column names, so we have to manually provide those\n",
    "col_names = [\"sequence_id\", \"source\", \"feature\", \"start\", \"end\", \"score\", \"strand\", \"phase\", \"attributes\"]\n",
    "\n",
    "# read the GFF file into a Pandas dataframe\n",
    "AE004437_df = pd.read_csv(gff_filename, sep = '\\t', names = col_names, skiprows = 5, skipfooter = 1, engine = 'python')\n",
    "# gff files are tab-separated, so need to specify sep='\\t' because read_csv will assume sep=',' by default\n",
    "# the names = colnames argument allows us to set the column names on import\n",
    "# skiprows here skips the first n rows of the dataframe on import\n",
    "# skipfooter skips the last n lines of the file; the last line of the GFF file contains only hashtags (\"#\")\n",
    "# engine = 'python' prevents skipfooter from raising an error\n",
    "\n",
    "# examine the dataframe\n",
    "AE004437_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b96a5-f8cd-4f0d-bbca-0e101283f93d",
   "metadata": {},
   "source": [
    "### Quick counts and visualizations\n",
    "\n",
    "Python can, of course, also be used to visualize data. Here, we use the `seaborn` package for visualization.\n",
    "\n",
    "Plot a histogram of gene lengths from the GFF file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079011c-7091-4247-b051-a29104ea846b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use seaborn to generate a histogram of the gene lengths\n",
    "\n",
    "# first, create a new column gene_length in the dataframe\n",
    "AE004437_df[\"gene_length\"] = AE004437_df.end - AE004437_df.start\n",
    "print(AE004437_df)\n",
    "\n",
    "# now plot the relevant data in a histogram\n",
    "gene_length_plot = sns.histplot(data = AE004437_df[AE004437_df[\"feature\"] == \"gene\"], x = \"gene_length\")\n",
    "# data = AE004437_df[AE004437_df[\"feature\"] == \"gene\"] will subset the Pandas dataframe to only the rows where feature == \"gene\"\n",
    "# next add labels to the axes & a title\n",
    "gene_length_plot.set_title('Gene Lengths in AE004437')\n",
    "gene_length_plot.set_ylabel('Counts')\n",
    "gene_length_plot.set_xlabel('Gene lengths')\n",
    "\n",
    "\n",
    "# visualize the plot\n",
    "gene_length_plot\n",
    "\n",
    "# write the plot to a file using matplotlib\n",
    "plt.savefig(\"AE004437_gene_lengths.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce6640-e001-45d6-9b88-768d33652088",
   "metadata": {},
   "source": [
    "### Inspect attributes\n",
    "\n",
    "Let's see if the GFF file contains protein sequences, by running several operations:\n",
    "- Extract only the `attributes` column\n",
    "- On each row of this column, split the attributes into a list of individual character strings (for easier visual inspection)\n",
    "- Randomly sample 4 entries to see what they contain\n",
    "\n",
    "For **reproducibility**, every time one works with random numbers, the initial value (\"seed\") of the random number generator has to be set to a known value. (Computers cannot really generate random numbers, just number sequences that *look* random.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b573a55-15ce-496a-925d-01f3b375fe69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check for protein sequences in the GFF file\n",
    "\n",
    "# now filter the dataset & sample it\n",
    "sampled_df = AE004437_df[AE004437_df[\"feature\"] == \"gene\"].attributes.str.split(\";\").sample(n = 4, random_state = 42)\n",
    "# subset the dataframe as we did before\n",
    "# one benefit of not leaving spaces in column names is that you can then use the formatting used here\n",
    "# .attributes selects the \"attributes\" column of the subsetted dataframe\n",
    "# specify that the column contains string information with .str\n",
    "# split the strings into a list using the semicolons as the delimiter to split by\n",
    "# randomly sample 4 rows from the column with .sample(n = 4, random_state = 42)\n",
    "# the random_state argument to .sample sets the seed for reproducibility\n",
    "\n",
    "# check the results\n",
    "# ref: https://stackoverflow.com/questions/25351968/how-can-i-display-full-non-truncated-dataframe-information-in-html-when-conver\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    # using this method prevents runcation of the column contents based on width\n",
    "    display(sampled_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
